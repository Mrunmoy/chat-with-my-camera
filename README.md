# ðŸ“¸ Chat with camera


**A modular, real-time object detection system for your local network - with YOLOv8, ZeroMQ pub/sub, timeline storage, and a local LLM that lets you *****talk***** to your cameras.** 
It is in no way perfect and is basically a sandbox for experimenting with different technologies and programming languages and see how we can integrate them together to make something so much fun!

I had a webcam lying around and a few Unifi flex cameras, so I just used them to see if I can make something that can do smart detections and integrate with LLM to answer some questions. Well to my pleasant surprise, working on this project was way more fun than I expected it to be. I was super scared but at least made it to this point here. Hope you have fun.

- Fully local â€” no cloud fees
- Modular pub/sub -> easy to swap parts
- Explainable AI -> no hallucinated answers, only real detections
- Can possibly work on your Pi, Jetson

---

## Key Features

- YOLOv8 detection (Python)
- Webcam & RTSP support (OpenCV)
- ZeroMQ pub/sub decoupled pipeline
- Go backend: SQLite timeline, snapshot API, retention cleanup
- React frontend: multi-camera grid, single-camera view, timeline, chat
- Local LLM (Ollama): smart prompt extraction -> timeline lookup -> natural answers

---

## Architecture Overview

```mermaid
flowchart TD
    A["Cameras (Webcam / RTSP)<br/>(Python OpenCV)"]
    --> B["YOLOv8 Detector<br/>(Ultralytics, Python)"]
    B --> C["ZeroMQ Publisher"]
    C -->|"PUB/SUB"| D["Go Backend<br/>â€¢ SQLite Timeline<br/>â€¢ Retention<br/>â€¢ Snapshots API"]
    D --> E["SQLite DB"]
    D --> F["Local LLM<br/>(Ollama)"]
    D --> G["React Frontend<br/>â€¢ Dashboard<br/>â€¢ Timeline<br/>â€¢ Chat"]
```

---

## Repo Layout

```
chat-with-my-camera/
â”œâ”€â”€ config/           # YAML/Env config
â”œâ”€â”€ backend/          # Go API: timeline, retention, LLM chat
â”œâ”€â”€ frontend/         # React app: grid, detail, chat
â”œâ”€â”€ camera/           # Python webcam modules
â”œâ”€â”€ detection/        # YOLOv8 inference logic
â”œâ”€â”€ publisher/        # ZeroMQ pub/sub
â”œâ”€â”€ snapshots/        # Stored images for timeline
â”œâ”€â”€ utils/            # Shared helpers
â”œâ”€â”€ main.py           # Python entry: camera + detection
â”œâ”€â”€ IDEAS.md          # Rolling roadmap
â””â”€â”€ README.md         # This file (overview)
```

---

## How it Works

- Python runs cameras + YOLOv8 -> publishes detections via ZeroMQ.
- Go backend subscribes -> logs detections in SQLite -> stores snapshots -> serves `/timeline` & `/snapshots`.
- Frontend shows grid + timeline -> lets you chat with the LLM:
*â€œWhen did you last see a delivery van?â€* ->
LLM extracts object -> backend queries timeline -> LLM returns real context answer.

---

## Quick Start

**Python detector + publisher:**

```bash
python3 -m venv venv && source venv/bin/activate
pip install -r requirements.txt
python main.py
```

**Go Backend:**

```bash
cd backend
go run main.go
```

**React Frontend:**

```bash
cd frontend
npm install
npm run dev
```

**Local LLM (Ollama):**

```bash
ollama serve  # Run Ollama on localhost:11434
ollama pull llama3
```

---


Each piece has its own README:

- [`backend/README.md`](./backend/README.md)  â€” API, timeline, retention, LLM chat loop
- [`frontend/README.md`](./frontend/README.md)  â€” dashboard structure, components, styles
- [`python/README.md`](./python/README.md)  â€” detection loop, pub/sub config
- [`llm/README.md`](./llm/README.md)  â€” local Ollama usage, prompt examples

## Next Steps

See [`IDEAS.md`](./IDEAS.md) for whatâ€™s next.

---

## License

MIT
